{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "시작하기 전에 상단의 메뉴에서<br>\n",
        "**런타임 -> 런타임 유형 변경 -> 하드웨어 가속기**<br>\n",
        "에서 **T4 GPU**를 선택합니다.<br><br>\n",
        "그리고 왼쪽 끝의 맨 위에 점과 선이 세개있는 아이콘을 클릭하면 목차를 보면서 실습 가능합니다."
      ],
      "metadata": {
        "id": "BGF9L9rNOeNA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. kobart모델 사용해보기\n",
        "kobart는 자연어 그대로를 이해할 수는 없습니다. 사실 이것은 kobart뿐만 아니라 모든 인공지능 모델들도 마찬가지입니다. 다행히도 kobart를 포함한 모든 인공지능 모델들은 **자신들만의 tokenizer**를 제공합니다. 우리는 이 tokenizer를 통해 자연어를 인공지능 모델들이 이해 가능한 벡터로 변환할 수도 있고, 반대로 인공지능 모델들의 출력(벡터 형태임)을 자연어로 변환할 수도 있습니다."
      ],
      "metadata": {
        "id": "ve9ZHjVmROsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1. kobart 를 사용하기 위한 tokenizer와 model을 huggingface🤗에서 다운로드\n",
        "**kobart모델 설명**<br>\n",
        "kobart는 엄청 긴 글을 요약하는 인공지능 모델입니다. <br><br>\n",
        "**tokenizer 설명**<br>\n",
        "tokenizer는 자연어를 kobart 인공지능 모델이 이해할 수 있는 벡터로 변환하기도 하고, 반대로 kobart 모델이 만든 벡터 결과물을 자연어로 변환합니다. 자세한건 다음 코드블럭에서 설명합니다."
      ],
      "metadata": {
        "id": "cZTmXMkK7nLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n",
        "\n",
        "# 토크나이저와 모델을 불러옵니다.\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"EbanLee/kobart-summary-v3\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"EbanLee/kobart-summary-v3\")\n"
      ],
      "metadata": {
        "id": "0uSMp3ug7eWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 자연어를 벡터로 변환하기\n",
        "아래의 예제를 보면, kobart의 tokenizer를 사용하여 자연어를 변환하면 *input_ids*와 *attention_mask*두가지 벡터를 반환하는 것을 알 수 있습니다. <br><br> 각각의 벡터가 어떤 역할을 하는지는 지금 중요하지 않습니다.(나도 모름)\n",
        "<br><br> 하지만 이렇게 변환한 두 벡터를 어떻게 kobart에게 입력값으로 주는 지는 중요합니다. kobart에게 이 두벡터를 입력으로 주는 법은 다음 코드블럭에서 다룹니다."
      ],
      "metadata": {
        "id": "dog8a27cym8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer를 통해 자연어->벡터 Encoding\n",
        "input_text = \"10년 논란 끝에 밑글씨까지 새기고 제작 완료를 눈앞에 둔 ‘광화문 현판’을 원점에서 재검토해 한글 현판으로 교체하자는 주장이 문화계 명사들이 포함된 자칭 ‘시민모임’에서 나왔다.\\n  이들은 문화재청이 지난해 8월 최종 확정한 복원안이 시민 의견과 시대정신을 반영한 것이 아니라면서 오는 한글날까지 대대적인 현판 교체 시민운동을 벌이겠다고 예고했다.\\n  ‘광화문 현판 훈민정음체로 시민모임’(공동대표 강병인‧한재준, 이하 ‘시민모임’)에 이름을 올린 문화예술인은 현재까지 총 24명.\\n  이 중엔 2014~2016년 서울시 총괄건축가를 지낸 승효상 이로재 대표와 ‘안상수체’로 유명한 안상수 파주타이포그라피학교 교장, 유영숙 전 환경부장관(세종사랑방 회장), 임옥상 미술가 등이 있다.\\n  공동대표인 강병인 작가는 ‘참이슬’ ‘화요’ 등의 상표 글씨로 유명한 캘리그라피(서체) 작가다.\\n  ‘시민모임’은 14일 오후 서울 종로구의 한 서점에서 기자간담회를 열고 이 같은 입장과 함께 훈민정음 해례 글자꼴로 시범 제작한 모형 현판(1/2 크기 축소판)도 공개할 예정이다.\\n  강 공동대표는 13일 기자와 통화에서 “새 현판 제작 과정에서 한글로 만들자는 의견은 묵살됐다”면서 “지난해 8월 이후 문화재청에 거듭 입장을 전했지만 반영되지 않아 시민운동에 나서기로 했다”고 말했다.\\n  일단 문화예술인 주축으로 꾸렸지만 조만간 한글협회 등 한글 관련단체들과 연대한다는 방침이다.\\n  이들이 배포한 사전자료엔 ^한자현판 설치는 중국의 속국임을 표시하는 것으로 대한민국 정체성에 도움이 되지 않고 ^광화문은 21세기의 중건이지 복원이 아니므로 당대의 시대정신인 한글로 현판을 써야하며 ^한글현판은 미래에 남겨줄 우리 유산을 재창조한다는 의미라는 주장이 담겼다.\\n  현재 광화문 현판에 대해선 “고종이 경복궁을 중건할 때 당시 훈련대장이던 임태영이 쓴 광화문 현판의 글씨를 조그만 사진에서 스캐닝하고 이를 다듬어 이명박정부 때 설치된 것”이라면서 복원 기준으로서의 정당성을 깎아내렸다.\\n    ‘시민모임’에 참여한 승효상 대표도 개인의견을 전제로 “현판을 꼭 한가지만 고집할 필요도 없다.\\n  매년 교체할 수도 있고, 광장에서 보이는 정면엔 한글현판, 반대편엔 한자현판을 다는 아이디어도 가능한 것 아니냐”고 말했다.\\n  그러면서 “문화재 전문가들은 보수적일 수밖에 없지만 현판이란 게 요즘 말로는 ‘간판’인데 새 시대에 맞게 바꿔 다는 게 바람직하다”고 주장했다.\\n\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=1026) # 매개변수들의 의미는 지금 안중요함\n",
        "print(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FakEL5ILQqL4",
        "outputId": "70052d20-b457-46f4-bb7a-fbc3db7af0b2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[    0, 16889, 15289,  ...,     3,     3,     3]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 벡터로 변환한 자연어를 인공지능의 입력으로 주고 출력 받아보기\n",
        "이번 코드에서는 변환한 두 벡터를 kobart의 입력으로 주고 출력을 받아봅니다.\n",
        "1번 코드블럭에서 우리는 *model*변수에 *EbanLee/kobart-summary-v3* 모델을 담았습니다. 아래의 코드에서는 이러한 *model*변수의 *model.generate*함수를 통해 kobart모델에게 입력을 주고 그에 해당하는 출력을 받아봅니다. <br><br>\n",
        "model.generate함수는 input_ids와 attention_mask를 입력으로 받고, 벡터형태의 결과를 출력합니다."
      ],
      "metadata": {
        "id": "B4QglLDWys3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 변환한 두 벡터를 kobart에게 입력값으로 주기\n",
        "summary_text_ids = model.generate(\n",
        "input_ids=inputs['input_ids'], # 자연어를 tokenizer로 변환한 결과 1\n",
        "attention_mask=inputs['attention_mask'], # 자연어를 tokenizer로 변환한 결과 2\n",
        "bos_token_id=model.config.bos_token_id,\n",
        "eos_token_id=model.config.eos_token_id,\n",
        "length_penalty=1.0,\n",
        "max_length=300,\n",
        "min_length=12,\n",
        "num_beams=6,\n",
        "repetition_penalty=1.5,\n",
        "no_repeat_ngram_size=15,\n",
        ")\n",
        "# kobart의 출력을 print해보기 (너무 길기 때문에 첫 100개만 출력하게함)\n",
        "print(summary_text_ids[0][:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUhmBjXjxGAp",
        "outputId": "c8975dd7-89a6-4938-9873-9bebae1dd797"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([    1,     0, 24833, 14124, 13372, 19209, 10748, 12141, 12013, 21067,\n",
            "        14822, 25158, 11786, 16634, 18677, 23173, 12037, 15129, 16237, 22300,\n",
            "        19311, 14181, 15066, 22178, 16789, 14822, 15184,  9120, 16687, 12141,\n",
            "        16187, 27003, 14186, 14484, 14125, 27692, 14124, 13372, 17541, 14822,\n",
            "        21055, 14485, 12034, 16330, 18185, 15615,     1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 kobart의 출력을 tokenizer를 사용해서 자연어로 변환하기\n",
        "벡터 형태의 kobart의 출력을 tokenizer.decode함수를 통해 다시 자연어로 변환합니다. 자연어로 변환한 결과를 보면 kobart모델이 성공적으로 긴 글을 짧게 요약한 것을 알 수 있습니다."
      ],
      "metadata": {
        "id": "D9Lb_jg83LyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# kobart의 출력을 tokenizer로 다시 decoding해서 자연어로 바꾸기\n",
        "result = tokenizer.decode(summary_text_ids[0], skip_special_tokens=True) #tokenizer.decode를 통해 벡터를 자연어로 변환\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEo8PGY91B_t",
        "outputId": "77e5cfc3-84b1-46de-8eec-986122958a1d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "광화문 현판 훈민정음체로 시민모임에 이름을 올린 문화예술인 24명은 문화재청이 확정한 복원안이 시민 의견과 시대정신을 반영한 것이 아니라면서 대대적인 현판 교체 시민운동을 벌이겠다고 예고했다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. kobart 모델 학습시켜보기\n",
        "이번엔 단순히 사용해보는 것이 아니라, 우리가 직접 kobart모델을 학습시켜봅니다. <br>\n"
      ],
      "metadata": {
        "id": "2VYJoESj5jT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 kobart모델과 kobart의 tokenizer를 huggingface🤗에서 다운로드"
      ],
      "metadata": {
        "id": "vwCFZwKQ7A_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration, AdamW\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "\n",
        "# 1. 모델과 토크나이저 불러오기\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"EbanLee/kobart-summary-v3\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"EbanLee/kobart-summary-v3\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSfKSvl06VCl",
        "outputId": "3dd849dd-302d-41b1-8eaf-6b0ff7054903"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 학습시킬 데이터 정의\n",
        "인공지능을 학습 시키는 방법에는  [지도학습, 비지도학습, 강화학습](https://sungwookkang.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-%ED%95%99%EC%8A%B5-%EB%B0%A9%EB%B2%95Supervised-Unsupervised-Reinforcement)이 있습니다.(개념이 쉽고 지금은 안중요함, 궁금하면 링크 눌러봐)<br>\n",
        "kobart는 이중에서 지도학습으로 학습시키는 모델입니다. 그러므로 kobart를 학습시킬때는 입력값과 입력값에 해당하는 정답 값을 제공해야합니다. <br>\n",
        "아래의 데이터는 kobart을 학습시킬 때 사용할 입력값과 정답값을 한쌍씩 묶어서 총 10쌍을 만들어 놓은 것입니다. 보통 이런 학습용 데이터는 사람이 하나씩 만듭니다.<br>(윗줄은 긴문장, 아랫줄은 그 문장을 요약한 문장)"
      ],
      "metadata": {
        "id": "aXsgS6zm8ABJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = [\n",
        "    (\n",
        "        \"10년 논란 끝에 밑글씨까지 새기고 제작 완료를 눈앞에 둔 ‘광화문 현판’을 원점에서 재검토해 한글 현판으로 교체하자는 주장이 문화계 명사들이 포함된 자칭 ‘시민모임’에서 나왔다.\",\n",
        "        \"광화문 현판을 한글로 교체하자는 주장이 제기되다.\"\n",
        "    ),\n",
        "    (\n",
        "        \"문화재청이 지난해 8월 최종 확정한 복원안이 시민 의견과 시대정신을 반영한 것이 아니라는 비판이 제기되었다.\",\n",
        "        \"광화문 현판 복원안이 시민 의견을 반영하지 않았다는 비판.\"\n",
        "    ),\n",
        "    (\n",
        "        \"‘광화문 현판 훈민정음체로 시민모임’이 한글 현판 교체를 위한 시민운동을 벌이겠다고 밝혔다.\",\n",
        "        \"시민모임, 한글 현판 교체 운동 예고.\"\n",
        "    ),\n",
        "    (\n",
        "        \"서울시 총괄건축가를 지낸 승효상 이로재 대표 등 문화예술인 24명이 한글 현판 교체를 주장했다.\",\n",
        "        \"승효상 등 문화예술인 24명, 한글 현판 교체 주장.\"\n",
        "    ),\n",
        "    (\n",
        "        \"강병인 캘리그라피 작가는 현판을 한글로 제작해야 한다고 주장했다.\",\n",
        "        \"강병인 작가, 현판의 한글 제작 주장.\"\n",
        "    ),\n",
        "    (\n",
        "        \"한글협회 등 한글 관련 단체들과 연대해 한글 현판 교체 운동을 펼칠 계획이다.\",\n",
        "        \"한글 관련 단체와 연대해 한글 현판 교체 운동 예정.\"\n",
        "    ),\n",
        "    (\n",
        "        \"한자현판 설치는 중국의 속국임을 나타내는 것이라는 비판이 나왔다.\",\n",
        "        \"한자현판 설치는 중국의 속국 표현이라는 비판.\"\n",
        "    ),\n",
        "    (\n",
        "        \"광화문 현판을 21세기 시대정신에 맞춰 한글로 제작해야 한다는 주장이 제기되었다.\",\n",
        "        \"광화문 현판을 21세기에 맞게 한글로 제작해야 한다는 주장.\"\n",
        "    ),\n",
        "    (\n",
        "        \"승효상 대표는 현판을 한글과 한자로 매년 교체하는 것도 고려할 수 있다고 말했다.\",\n",
        "        \"승효상 대표, 현판을 매년 교체하자는 의견 제시.\"\n",
        "    ),\n",
        "    (\n",
        "        \"문화재 전문가들은 보수적이지만 현판을 시대에 맞게 바꾸는 것이 바람직하다는 주장이 나왔다.\",\n",
        "        \"현판을 시대에 맞게 바꾸자는 주장 제기.\"\n",
        "    )\n",
        "]\n"
      ],
      "metadata": {
        "id": "mD1rUnflRJPD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 학습에 필요한 다양한 함수&class정의\n",
        "**이해할 필요 없음**. (어차피 이번 학기에 인공지능하고 기계학습 강의에서 다 배울 내용들임. 지금은 무시무시해보여도 이번학기 끝나면 다 당연한 내용들이 될거임)그래도 간략하게 설명하자면\n",
        "\n",
        "*   *preprocess_data*함수는 tokenizer를 통해 데이터를 벡터의 형태로 변환하는 전처리과정을 거치는 함수이고,\n",
        "*   *SummaryDataset(Dataset)* 는 전처리를 거친 벡터 형태의 데이터들을 사용하기 편하게 묶어둔 객체이고,\n",
        "*   *train_dataset*는 SummaryDataset(Dataset)를 변수로 할당한 것이고,\n",
        "*   *train_dataloader*는 학습 능률을 위해서 데이터를 4개의 [mini-batch로](https://wikidocs.net/55580) 쪼개고 순서를 섞어주는 함수이다 (궁금하면 들어가봐)\n",
        "*   *optimizer*는 학습할때 설정하는 매개변수들을 최적화하는 함수이다. [optimizer공부](https://wikidocs.net/55580)(역시, 궁금하면 들어가봐)\n",
        "\n"
      ],
      "metadata": {
        "id": "fsPIBdyy_GyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. 데이터 전처리 함수 정의\n",
        "def preprocess_data(tokenizer, data, max_input_length=1024, max_target_length=128):\n",
        "    inputs = []\n",
        "    targets = []\n",
        "\n",
        "    for input_text, summary_text in data:\n",
        "        # 입력 텍스트 전처리\n",
        "        input_encodings = tokenizer(\n",
        "            input_text,\n",
        "            max_length=max_input_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # 요약 텍스트 전처리\n",
        "        target_encodings = tokenizer(\n",
        "            summary_text,\n",
        "            max_length=max_target_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        inputs.append(input_encodings)\n",
        "        targets.append(target_encodings)\n",
        "\n",
        "    return inputs, targets\n",
        "\n",
        "inputs, targets = preprocess_data(tokenizer, train_data)\n",
        "\n",
        "# 4. 커스텀 데이터셋 정의\n",
        "class SummaryDataset(Dataset):\n",
        "    def __init__(self, inputs, targets):\n",
        "        self.inputs = inputs\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_ids = self.inputs[idx]['input_ids'].squeeze()\n",
        "        attention_mask = self.inputs[idx]['attention_mask'].squeeze()\n",
        "        labels = self.targets[idx]['input_ids'].squeeze()\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels,\n",
        "        }\n",
        "\n",
        "# 5. 데이터로더 생성\n",
        "train_dataset = SummaryDataset(inputs, targets)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "# 6. 옵티마이저 설정\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ],
      "metadata": {
        "id": "poQuDB45RNuJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 본격적으로 모델 학습시켜보기 (오래걸림)\n",
        "**num_epochs**는 우리가 우리의 데이터를 몇번 학습 시킬것인지. (현재는 4이므로 우리의 데이터를 4번 학습시킴)<br><br><br> **학습 과정 설명**\n",
        "\n",
        "\n",
        "1.   입력 데이터를 통해 현재 우리의 모델의 예측을 얻음(outputs)\n",
        "2.   현재 우리 모델의 예측이 정답과 얼마나 다른지 계산. 예측과 정답의 차이는 loss값임. 작을수록 좋은것이다.\n",
        "3. loss값에 따른 *오차역전파*(이것도 이번학기 초반에 배울것임)를 통해 우리의 모델을 학습시킴\n",
        "4. 이 과정을 epoch(4번)만큼 반복 (epoch가 진행됨에 따라 loss가 줄어드는 것을 볼 수 있다. 이것은 우리 모델이 학습하면서 정답과 예측 사이의 오차가 줄어들고 있다는 것이다)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5tFIloWjCnGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. 모델 학습\n",
        "model.train()\n",
        "num_epochs = 4 # 데이터를 몇번 학습시킬 것인지\n",
        "\n",
        "for epoch in range(num_epochs): # 총 3번 반복\n",
        "    for batch in train_dataloader: # train_dataloader는 우리의 데이터를 4개로 작게 쪼개놓은 배열\n",
        "        optimizer.zero_grad() # 안중요. 그냥 다양한 매개변수를 최적화\n",
        "\n",
        "        # 배치 데이터 추출\n",
        "        input_ids = batch[\"input_ids\"] # kobart가 필요로하는 첫번째 벡터 input_id\n",
        "        attention_mask = batch[\"attention_mask\"] # kobart가 필요로하는 첫번째 벡터 attention_mask\n",
        "        labels = batch[\"labels\"] # 우리의 데이터에서 정답값을 tokenizer를 통해 벡터로 바꿔둔 것 중 input_ids\n",
        "\n",
        "        # 모델 예측\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        # 손실 계산 및 역전파\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1} completed. Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8t2Rhb6CJmy",
        "outputId": "5938ff74-bed1-40d2-ad51-63e5c7565d7a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 completed. Loss: 0.1379823535680771\n",
            "Epoch 2 completed. Loss: 0.04229949414730072\n",
            "Epoch 3 completed. Loss: 0.021657351404428482\n",
            "Epoch 4 completed. Loss: 0.020212840288877487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 학습시킨 모델 저장하기\n",
        "지금까지 학습시킨 모델을 저장합니다. 아래의 코드를 시행하면 같은 디렉터리에 kobart_summary_model폴더와 kobart_summary_tokenizer폴더가 생성됩니다. (만약 구글 colab에서 실습하고 있으면 왼쪽 끝의 메뉴에서 폴더 모양을 누르면 확인 가능함)"
      ],
      "metadata": {
        "id": "GzYiePVcKwQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. 학습이 완료된 모델을 저장 (선택 사항)\n",
        "model.save_pretrained(\"./kobart_summary_model\")\n",
        "tokenizer.save_pretrained(\"./kobart_summary_tokenizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-veLq7tEeTk",
        "outputId": "bcb3b3ac-fd43-4375-d243-d7fac5185080"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'forced_eos_token_id': 1}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./kobart_summary_tokenizer/tokenizer_config.json',\n",
              " './kobart_summary_tokenizer/special_tokens_map.json',\n",
              " './kobart_summary_tokenizer/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.6 저장한 모델 불러와서 써보기\n",
        "우리가 학습시켜서 같은 디렉터리에 저장했던 kobart와 tokenizer를 불러와서 사용합니다."
      ],
      "metadata": {
        "id": "VorfaaHoLd51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n",
        "\n",
        "# 1. 저장된 모델과 토크나이저 불러오기\n",
        "model_path = \"./kobart_summary_model\"\n",
        "tokenizer_path = \"./kobart_summary_tokenizer\"\n",
        "\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(tokenizer_path)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_path)\n",
        "\n",
        "# 2. 새로운 입력 텍스트 준비\n",
        "input_text = '''\n",
        " 우리 국민 10명 중 7명은 뉴스를 회피하고 있다는 조사 결과가 나왔다. 뉴스 회피를 빚는 주된 상황은 ‘정치적인 사건, 이슈들이 너무 많을 때’, ‘반복적으로 너무 많은 뉴스가 쏟아져 나올 때’, ‘보고 싶지 않은 인물이 뉴스에 나올 때’ 등이 꼽혔다.\n",
        "22일 한국언론진흥재단이 발간한 미디어이슈(10권4호)를 보면 뉴스 회피 응답률은 72.1%에 이르는 것으로 나타났다. 뉴스 회피와 정치적 성향 간의 관계를 살펴보면, 스스로를 보수라고 응답한 이들이 진보라고 응답한 이들보다 뉴스 회피를 상대적으로 더 많이 하는 것으로 조사됐다. 구체적으로는 ‘매우 보수’라고 응답한 이들 중 76.6%, ‘대체로 보수’ 74.2%, ‘중도’ 72.9%, ‘대체로 진보’ 67.4%, ‘매우 진보’ 66.7%가 뉴스 회피를 한 적이 있다고 했다.\n",
        "'''\n",
        "\n",
        "# 3. 입력 텍스트 토큰화\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=1024)\n",
        "\n",
        "# 4. 모델을 사용하여 요약 생성\n",
        "summary_ids = model.generate(\n",
        "    input_ids=inputs['input_ids'],\n",
        "    attention_mask=inputs['attention_mask'],\n",
        "    max_length=150,\n",
        "    min_length=30,\n",
        "    num_beams=4,\n",
        "    length_penalty=2.0,\n",
        "    early_stopping=True\n",
        ")\n",
        "\n",
        "# 5. 생성된 요약 디코딩\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPfOVG1ymp3g",
        "outputId": "dbce9efa-f15a-4794-aa2d-7028b87ba059"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "우리 국민 10명 중 7명은 뉴스를 회피하고 있다는 조사 결과가 나왔다. 뉴스 회피를 빚는 주된 상황은 정치적 사건, 이슈들이 너무 많을 때 등이다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. kobart 사용 코드 (프롬프트 엔지니어링 실습용)\n",
        "input_text에다가 네가 실험해보고 싶은 문장을 넣으면, 그 문장을 요약해서 출력합니다."
      ],
      "metadata": {
        "id": "MFdqbcRhPtM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration\n",
        "\n",
        "# 토크나이저와 모델을 불러옵니다.\n",
        "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"EbanLee/kobart-summary-v3\")\n",
        "model = BartForConditionalGeneration.from_pretrained(\"EbanLee/kobart-summary-v3\")\n",
        "\n",
        "# Tokenizer를 통해 자연어->벡터 Encoding\n",
        "input_text = \"여기에 네가 조립한 프롬프트를 넣으면됩니다.\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=1026) # 매개변수들의 의미는 지금 안중요함\n",
        "print(inputs)\n",
        "\n",
        "# 변환한 두 벡터를 kobart에게 입력값으로 주기\n",
        "summary_text_ids = model.generate(\n",
        "input_ids=inputs['input_ids'], # 자연어를 tokenizer로 변환한 결과 1\n",
        "attention_mask=inputs['attention_mask'], # 자연어를 tokenizer로 변환한 결과 2\n",
        "bos_token_id=model.config.bos_token_id,\n",
        "eos_token_id=model.config.eos_token_id,\n",
        "length_penalty=1.0,\n",
        "max_length=300,\n",
        "min_length=12,\n",
        "num_beams=6,\n",
        "repetition_penalty=1.5,\n",
        "no_repeat_ngram_size=15,\n",
        ")\n",
        "\n",
        "# kobart의 출력을 tokenizer로 다시 decoding해서 자연어로 바꾸기\n",
        "result = tokenizer.decode(summary_text_ids[0], skip_special_tokens=True) #tokenizer.decode를 통해 벡터를 자연어로 변환\n",
        "print(result)"
      ],
      "metadata": {
        "id": "qgrgpamiQKhE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}